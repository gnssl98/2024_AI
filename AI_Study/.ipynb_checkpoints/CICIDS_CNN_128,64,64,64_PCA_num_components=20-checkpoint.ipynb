{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4b0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gnssl\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5999a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28e0489",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anchanghun/Downloads/Winter_AI_Study/dataset/CIC-IDS-2017/MachineLearningCVE/combiSCX.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/anchanghun/Downloads/Winter_AI_Study/dataset/CIC-IDS-2017/MachineLearningCVE/combiSCX.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anchanghun/Downloads/Winter_AI_Study/dataset/CIC-IDS-2017/MachineLearningCVE/combiSCX.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/anchanghun/Downloads/Winter_AI_Study/dataset/CIC-IDS-2017/MachineLearningCVE/combiSCX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Flow Bytes/s\"] = df[\"Flow Bytes/s\"].fillna(df[\"Flow Bytes/s\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Flow Bytes/s\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571cfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b342e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f917cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = int((nCol + nGraphPerRow - 1) / nGraphPerRow)\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plotPerColumnDistribution(df, 79, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5168379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(df):\n",
    "    outliers_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype != 'object':\n",
    "            std = df[column].std()\n",
    "            if std > 0:\n",
    "                z_score = (df[column] - df[column].mean()) / std\n",
    "                if (np.abs(z_score) > 3).any():\n",
    "                    outliers_columns.append(column)\n",
    "        return outliers_columns\n",
    "\n",
    "outliers_columns = find_outliers(df)\n",
    "print(\"Columns with outliers (3 standard deviations or more away from the mean):\", outliers_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d570188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_invalid_columns(df):\n",
    "    invalid_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype != object:  # 문자열이 아닌 경우에만 확인합니다.\n",
    "            if np.any(np.isinf(df[column])) or np.any(np.abs(df[column]) > 1e10 ):\n",
    "                invalid_columns.append(column)\n",
    "    return invalid_columns\n",
    "\n",
    "# 데이터프레임 df에서 불가능한 값을 포함하는 컬럼 찾기\n",
    "invalid_cols = find_invalid_columns(df)\n",
    "\n",
    "if invalid_cols:\n",
    "    print(\"The following columns contain invalid values:\")\n",
    "    print(invalid_cols)\n",
    "else:\n",
    "    print(\"No columns contain invalid values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = df[\" Destination Port\"]\n",
    "#df = df.drop(columns=\" Destination Port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97018fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by='Flow Bytes/s', ascending=False)\n",
    "print(sorted_df['Flow Bytes/s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36141623",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e8  # 예시 임계값\n",
    "inf_col = ['Flow Bytes/s', ' Flow Packets/s', ' Fwd Header Length', ' Fwd Header Length.1']\n",
    "\n",
    "for col in inf_col:\n",
    "    df[col] = np.clip(df[col], -threshold, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97da0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split dataset on train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test=train_test_split(df,test_size=0.3, random_state=10)\n",
    "\n",
    "#Exploratory Analysis\n",
    "# Descriptive statistics\n",
    "train.describe()\n",
    "test.describe()\n",
    "\n",
    "# Packet Attack Distribution\n",
    "train[' Label'].value_counts()\n",
    "test[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# extract numerical attributes and scale it to have zero mean and unit variance  \n",
    "cols = train.select_dtypes(include=['float64','int64']).columns\n",
    "sc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))\n",
    "sc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))\n",
    "\n",
    "# turn the result back to a dataframe\n",
    "sc_traindf = pd.DataFrame(sc_train, columns = cols)\n",
    "sc_testdf = pd.DataFrame(sc_test, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder from sklearn \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder() \n",
    "\n",
    "trainDep = train[' Label'].values.reshape(-1,1)\n",
    "trainDep = onehotencoder.fit_transform(trainDep).toarray()\n",
    "testDep = test[' Label'].values.reshape(-1,1)\n",
    "testDep = onehotencoder.fit_transform(testDep).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=sc_traindf\n",
    "train_y=trainDep[:,0]\n",
    "\n",
    "test_X=sc_testdf\n",
    "test_y=testDep[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ee3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming train_X and test_X are your feature matrices, and train_y and test_y are your target vectors\n",
    "\n",
    "# Standardize the data (important before applying PCA)\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Apply PCA\n",
    "num_components = 20  # Use 20 components for PCA\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_X_pca = pca.fit_transform(train_X_scaled)\n",
    "\n",
    "# Transform the testing data (using the same PCA transformation)\n",
    "test_X_pca = pca.transform(test_X_scaled)\n",
    "\n",
    "# Select top 20 features using SelectKBest with the f_classif scoring function\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_X_selected = feature_selector.fit_transform(train_X_pca, train_y)\n",
    "\n",
    "# Transform the testing data (using the same feature selection)\n",
    "test_X_selected = feature_selector.transform(test_X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "# Assuming train_X_selected and test_X_selected are your selected features after PCA and feature selection\n",
    "\n",
    "# Specify the number of selected features\n",
    "num_selected_features = train_X_selected.shape[1]\n",
    "\n",
    "# Reshape the data for 1D CNN (assuming you have time series-like data)\n",
    "train_X_selected = train_X_selected.reshape(train_X_selected.shape[0], num_selected_features, 1)\n",
    "test_X_selected = test_X_selected.reshape(test_X_selected.shape[0], num_selected_features, 1)\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(num_selected_features, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(num_selected_features, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(num_selected_features, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten layer to connect convolutional layers to dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Additional hidden layers\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for your task\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_X_selected, train_y, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_X_selected, test_y)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test set\n",
    "predictions = model.predict(test_X_selected)\n",
    "predictions = (predictions > 0.5)\n",
    "\n",
    "conf_matrix = confusion_matrix(test_y, predictions)\n",
    "TN = conf_matrix[0, 0]  # True Negative\n",
    "FP = conf_matrix[0, 1]  # False Positive\n",
    "TP = conf_matrix[1, 1]  # True Positive\n",
    "FN = conf_matrix[1, 0]  # False Negative\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "\n",
    "# Calculate additional evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_y, predictions))\n",
    "\n",
    "print(\"\\nSpecificity:\")\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e095c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 기존 코드\n",
    "report_dict = classification_report(test_y, predictions, output_dict=True)\n",
    "\n",
    "# 소수점 5자리까지 출력\n",
    "print(\"Classification Report (소수점 5자리):\")\n",
    "for key, value in report_dict.items():\n",
    "    if isinstance(value, dict):\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"{key} {sub_key}: {sub_value:.5f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
