{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5999a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\dataset\\CIC-IDS-2017\\MachineLearningCVE\\combISCX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Flow Bytes/s\"] = df[\"Flow Bytes/s\"].fillna(df[\"Flow Bytes/s\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Flow Bytes/s\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571cfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b342e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f917cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = int((nCol + nGraphPerRow - 1) / nGraphPerRow)\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plotPerColumnDistribution(df, 79, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5168379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(df):\n",
    "    outliers_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype != 'object':\n",
    "            std = df[column].std()\n",
    "            if std > 0:\n",
    "                z_score = (df[column] - df[column].mean()) / std\n",
    "                if (np.abs(z_score) > 3).any():\n",
    "                    outliers_columns.append(column)\n",
    "        return outliers_columns\n",
    "\n",
    "outliers_columns = find_outliers(df)\n",
    "print(\"Columns with outliers (3 standard deviations or more away from the mean):\", outliers_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d570188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_invalid_columns(df):\n",
    "    invalid_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype != object:  # 문자열이 아닌 경우에만 확인합니다.\n",
    "            if np.any(np.isinf(df[column])) or np.any(np.abs(df[column]) > 1e10 ):\n",
    "                invalid_columns.append(column)\n",
    "    return invalid_columns\n",
    "\n",
    "# 데이터프레임 df에서 불가능한 값을 포함하는 컬럼 찾기\n",
    "invalid_cols = find_invalid_columns(df)\n",
    "\n",
    "if invalid_cols:\n",
    "    print(\"The following columns contain invalid values:\")\n",
    "    print(invalid_cols)\n",
    "else:\n",
    "    print(\"No columns contain invalid values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = df[\" Destination Port\"]\n",
    "#df = df.drop(columns=\" Destination Port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97018fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by='Flow Bytes/s', ascending=False)\n",
    "print(sorted_df['Flow Bytes/s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36141623",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e8  # 예시 임계값\n",
    "inf_col = ['Flow Bytes/s', ' Flow Packets/s', ' Fwd Header Length', ' Fwd Header Length.1']\n",
    "\n",
    "for col in inf_col:\n",
    "    df[col] = np.clip(df[col], -threshold, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97da0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split dataset on train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test=train_test_split(df,test_size=0.3, random_state=10)\n",
    "\n",
    "#Exploratory Analysis\n",
    "# Descriptive statistics\n",
    "train.describe()\n",
    "test.describe()\n",
    "\n",
    "# Packet Attack Distribution\n",
    "train[' Label'].value_counts()\n",
    "test[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# extract numerical attributes and scale it to have zero mean and unit variance  \n",
    "cols = train.select_dtypes(include=['float64','int64']).columns\n",
    "sc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))\n",
    "sc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))\n",
    "\n",
    "# turn the result back to a dataframe\n",
    "sc_traindf = pd.DataFrame(sc_train, columns = cols)\n",
    "sc_testdf = pd.DataFrame(sc_test, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder from sklearn \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder() \n",
    "\n",
    "trainDep = train[' Label'].values.reshape(-1,1)\n",
    "trainDep = onehotencoder.fit_transform(trainDep).toarray()\n",
    "testDep = test[' Label'].values.reshape(-1,1)\n",
    "testDep = onehotencoder.fit_transform(testDep).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=sc_traindf\n",
    "train_y=trainDep[:,0]\n",
    "\n",
    "test_X=sc_testdf\n",
    "test_y=testDep[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ee3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming train_X and test_X are your feature matrices, and train_y and test_y are your target vectors\n",
    "\n",
    "# Standardize the data (important before applying PCA)\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Apply PCA\n",
    "num_components = 30  # Use 10 components for PCA\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_X_pca = pca.fit_transform(train_X_scaled)\n",
    "\n",
    "# Transform the testing data (using the same PCA transformation)\n",
    "test_X_pca = pca.transform(test_X_scaled)\n",
    "\n",
    "# Select top 20 features using SelectKBest with the f_classif scoring function\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_X_selected = feature_selector.fit_transform(train_X_pca, train_y)\n",
    "\n",
    "# Transform the testing data (using the same feature selection)\n",
    "test_X_selected = feature_selector.transform(test_X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "# Assuming train_X_selected and test_X_selected are your selected features after PCA and feature selection\n",
    "\n",
    "# Specify the number of selected features\n",
    "num_selected_features = train_X_selected.shape[1]\n",
    "\n",
    "# Reshape the data for 1D CNN (assuming you have time series-like data)\n",
    "train_X_selected = train_X_selected.reshape(train_X_selected.shape[0], num_selected_features, 1)\n",
    "test_X_selected = test_X_selected.reshape(test_X_selected.shape[0], num_selected_features, 1)\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(num_selected_features, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(num_selected_features, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten layer to connect convolutional layers to dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Additional hidden layers\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for your task\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_X_selected, train_y, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_X_selected, test_y)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test set\n",
    "predictions = model.predict(test_X_selected)\n",
    "predictions = (predictions > 0.5)\n",
    "\n",
    "conf_matrix = confusion_matrix(test_y, predictions)\n",
    "TN = conf_matrix[0, 0]  # True Negative\n",
    "FP = conf_matrix[0, 1]  # False Positive\n",
    "TP = conf_matrix[1, 1]  # True Positive\n",
    "FN = conf_matrix[1, 0]  # False Negative\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "\n",
    "# Calculate additional evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_y, predictions))\n",
    "\n",
    "print(\"\\nSpecificity:\")\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e095c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 기존 코드\n",
    "report_dict = classification_report(test_y, predictions, output_dict=True)\n",
    "\n",
    "# 소수점 5자리까지 출력\n",
    "print(\"Classification Report (소수점 5자리):\")\n",
    "for key, value in report_dict.items():\n",
    "    if isinstance(value, dict):\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"{key} {sub_key}: {sub_value:.5f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
